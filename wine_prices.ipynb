{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pkrosoff/wine_machine_learning/blob/kelly_branch/wine_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEH6F0fWLgLl"
   },
   "source": [
    "Model to Predict Wine Price Range \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VH2QgC9r11HM",
    "outputId": "24d485ff-6bdc-415c-eea6-722f66ce4a95"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: $SPARK_VERSION-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0d414a36f2a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonAdv\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         raise Exception(\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         )\n\u001b[0;32m    148\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "spark_version = 'spark-3.0.1'\n",
    "# spark_version = 'spark-3.<enter version>'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t8_9nT_F5N1M"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"wine_hashing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pEQfRAGJ5cG5"
   },
   "outputs": [],
   "source": [
    "# Initial dependencies to import\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKPw6dJPL89n"
   },
   "source": [
    "BEFORE PROGRESSING ENSURE GOOGLE DRIVE IS MOUNTED TO NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mDUw_mUxGNPU"
   },
   "outputs": [],
   "source": [
    "# Read in the CSV from local Google Drive into Pandas Dataframe\n",
    "\n",
    "\n",
    "path = \"/content/drive/MyDrive/winemag-data_first150k.csv\"\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "y0EqeL4MXMyY",
    "outputId": "93d8309e-891e-42b4-f633-d70785fd99b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>Martha's Vineyard</td>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Heitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>Carodorum Selección Especial Reserva</td>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Toro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tinta de Toro</td>\n",
       "      <td>Bodega Carmen Rodríguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>Special Selected Late Harvest</td>\n",
       "      <td>96</td>\n",
       "      <td>90.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Knights Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Macauley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>96</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Ponzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>La Brûlade</td>\n",
       "      <td>95</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Provence</td>\n",
       "      <td>Bandol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provence red blend</td>\n",
       "      <td>Domaine de la Bégude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150925</th>\n",
       "      <td>150925</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Many people feel Fiano represents southern Ita...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Southern Italy</td>\n",
       "      <td>Fiano di Avellino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Feudi di San Gregorio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150926</th>\n",
       "      <td>150926</td>\n",
       "      <td>France</td>\n",
       "      <td>Offers an intriguing nose with ginger, lime an...</td>\n",
       "      <td>Cuvée Prestige</td>\n",
       "      <td>91</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>H.Germain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150927</th>\n",
       "      <td>150927</td>\n",
       "      <td>Italy</td>\n",
       "      <td>This classic example comes from a cru vineyard...</td>\n",
       "      <td>Terre di Dora</td>\n",
       "      <td>91</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Southern Italy</td>\n",
       "      <td>Fiano di Avellino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Terredora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150928</th>\n",
       "      <td>150928</td>\n",
       "      <td>France</td>\n",
       "      <td>A perfect salmon shade, with scents of peaches...</td>\n",
       "      <td>Grand Brut Rosé</td>\n",
       "      <td>90</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>Champagne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Champagne Blend</td>\n",
       "      <td>Gosset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150929</th>\n",
       "      <td>150929</td>\n",
       "      <td>Italy</td>\n",
       "      <td>More Pinot Grigios should taste like this. A r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Northeastern Italy</td>\n",
       "      <td>Alto Adige</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pinot Grigio</td>\n",
       "      <td>Alois Lageder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150930 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 country  ...             variety                   winery\n",
       "0                0      US  ...  Cabernet Sauvignon                    Heitz\n",
       "1                1   Spain  ...       Tinta de Toro  Bodega Carmen Rodríguez\n",
       "2                2      US  ...     Sauvignon Blanc                 Macauley\n",
       "3                3      US  ...          Pinot Noir                    Ponzi\n",
       "4                4  France  ...  Provence red blend     Domaine de la Bégude\n",
       "...            ...     ...  ...                 ...                      ...\n",
       "150925      150925   Italy  ...         White Blend    Feudi di San Gregorio\n",
       "150926      150926  France  ...     Champagne Blend                H.Germain\n",
       "150927      150927   Italy  ...         White Blend                Terredora\n",
       "150928      150928  France  ...     Champagne Blend                   Gosset\n",
       "150929      150929   Italy  ...        Pinot Grigio            Alois Lageder\n",
       "\n",
       "[150930 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUHthRh_Y5TG",
    "outputId": "d0113b87-702b-476d-cf67-83e23e835e76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NaN in every column\n",
      " Unnamed: 0         0\n",
      "country            5\n",
      "description        0\n",
      "designation    45735\n",
      "points             0\n",
      "price          13695\n",
      "province           5\n",
      "region_1       25060\n",
      "region_2       89977\n",
      "variety            0\n",
      "winery             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Examine the schema to find number of null values by column\n",
    "print(\"# of NaN in every column\\n\", df.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KUGdcsgZmI_Q"
   },
   "outputs": [],
   "source": [
    "# Select the 'description' and 'price' column to be used in analysis\n",
    "\n",
    "df_2 = df[['description', 'price']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXVlAjMRgVhf",
    "outputId": "1d24bafd-97ee-457c-b7ca-a965454f6d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150930 entries, 0 to 150929\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   description  150930 non-null  object \n",
      " 1   price        137235 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "df_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoeCpwfrg0_E",
    "outputId": "814d8453-cd9c-4d48-c207-321973c45af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NaN in every column\n",
      " description        0\n",
      "price          13695\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Secondary check of amount of NaN or Null values in DataFrame\n",
    "print(\"# of NaN in every column\\n\", df_2.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Vhphf8xCg1Xu",
    "outputId": "b0e49a73-0b7b-46ef-ca43-caa9cba8dd8a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150925</th>\n",
       "      <td>Many people feel Fiano represents southern Ita...</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150926</th>\n",
       "      <td>Offers an intriguing nose with ginger, lime an...</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150927</th>\n",
       "      <td>This classic example comes from a cru vineyard...</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150928</th>\n",
       "      <td>A perfect salmon shade, with scents of peaches...</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150929</th>\n",
       "      <td>More Pinot Grigios should taste like this. A r...</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137235 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              description  price\n",
       "0       This tremendous 100% varietal wine hails from ...  235.0\n",
       "1       Ripe aromas of fig, blackberry and cassis are ...  110.0\n",
       "2       Mac Watson honors the memory of a wine once ma...   90.0\n",
       "3       This spent 20 months in 30% new French oak, an...   65.0\n",
       "4       This is the top wine from La Bégude, named aft...   66.0\n",
       "...                                                   ...    ...\n",
       "150925  Many people feel Fiano represents southern Ita...   20.0\n",
       "150926  Offers an intriguing nose with ginger, lime an...   27.0\n",
       "150927  This classic example comes from a cru vineyard...   20.0\n",
       "150928  A perfect salmon shade, with scents of peaches...   52.0\n",
       "150929  More Pinot Grigios should taste like this. A r...   15.0\n",
       "\n",
       "[137235 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Null Values from DataFrame\n",
    "df_2 = df_2.dropna()\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "182ONnL6g1pD",
    "outputId": "12a3eb71-3649-4362-d0ec-f5fbcc9e9313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of NaN in every column\n",
      " description    0\n",
      "price          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure the Columns were dropped\n",
    "print(\"# of NaN in every column\\n\", df_2.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "DdpRmohCg150",
    "outputId": "f824d254-c36f-4ad5-df17-78411a6d7104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 137235 entries, 0 to 150929\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   description         137235 non-null  object \n",
      " 1   price               137235 non-null  float64\n",
      " 2   description_length  137235 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 4.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>description_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>235.0</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  price  description_length\n",
       "0  This tremendous 100% varietal wine hails from ...  235.0                 355\n",
       "1  Ripe aromas of fig, blackberry and cassis are ...  110.0                 318\n",
       "2  Mac Watson honors the memory of a wine once ma...   90.0                 280\n",
       "3  This spent 20 months in 30% new French oak, an...   65.0                 386\n",
       "4  This is the top wine from La Bégude, named aft...   66.0                 376"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add column for length of description for NLP processing model\n",
    "\n",
    "df_3 = df_2.assign(description_length = df_2['description'].apply(len))\n",
    "df_3.info()\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhUG2bimN6Xo"
   },
   "source": [
    "Five separate functions to create bins for separate price ranges of wine.\n",
    "For each time the code is run, all but one of the functions, and their accompanying code to create the \"price_grouped\" column, must be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ek8v61DQdvsR",
    "outputId": "f8938407-a325-4ce1-bf8e-efbe76b6363c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>description_length</th>\n",
       "      <th>price_grouped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>235.0</td>\n",
       "      <td>355</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>318</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>280</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>386</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>376</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  ...  price_grouped\n",
       "0  This tremendous 100% varietal wine hails from ...  ...              8\n",
       "1  Ripe aromas of fig, blackberry and cassis are ...  ...              5\n",
       "2  Mac Watson honors the memory of a wine once ma...  ...              3\n",
       "3  This spent 20 months in 30% new French oak, an...  ...              2\n",
       "4  This is the top wine from La Bégude, named aft...  ...              2\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bin the prices so the range of prices is more manageable for the model\n",
    "\n",
    "# BINS NUMBER: 1, Returns 74% Accuracy\n",
    "\n",
    "def price_bins(price):\n",
    "    if price < 35:\n",
    "        return 1\n",
    "    elif price >= 35 and price < 70:\n",
    "        return 2 \n",
    "    elif price >= 70 and price < 105:\n",
    "        return 3 \n",
    "    elif price >= 105 and price < 140:\n",
    "        return 5 \n",
    "    elif price >= 140 and price < 175:\n",
    "        return 6\n",
    "    elif price >= 175 and price < 210:\n",
    "        return 7\n",
    "    elif price >= 210 and price < 245:\n",
    "        return 8\n",
    "    elif price >= 245 and price < 280:\n",
    "        return 9\n",
    "    elif price >= 280 and price < 315:\n",
    "        return 10\n",
    "    elif price >= 315 and price < 350:\n",
    "        return 11\n",
    "    elif price >= 350 and price < 385:\n",
    "        return 12\n",
    "    elif price >= 385 and price < 420:\n",
    "        return 13\n",
    "    elif price >= 420 and price < 455:\n",
    "        return 14\n",
    "    elif price >= 455 and price < 490:\n",
    "        return 15\n",
    "    elif price >= 490 and price < 525:\n",
    "        return 16\n",
    "    elif price >= 525 and price < 560:\n",
    "        return 17\n",
    "    elif price >= 560 and price < 595:\n",
    "        return 18\n",
    "    elif price >= 595 and price < 630:\n",
    "        return 19\n",
    "    elif price >= 630 and price < 665:\n",
    "        return 20\n",
    "    elif price >= 665 and price < 700:\n",
    "        return 21\n",
    "    else:\n",
    "        return 22\n",
    "\n",
    "# Applying transform method and assigning result to new column \"price_grouped\"\n",
    "df_3 = df_3.assign(price_grouped = df_3['price'].apply(price_bins))\n",
    "df_3.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UAy-fNtChs0Y"
   },
   "outputs": [],
   "source": [
    "# # ALTERNATIVE BINS MODEL NUMBER: 2, Returns 83% Accuracy\n",
    "\n",
    "# def price_bins_2(price):\n",
    "#     if price < 50:\n",
    "#         return 1\n",
    "#     elif price >= 50 and price < 100:\n",
    "#         return 2\n",
    "#     elif price >= 100 and price < 150:\n",
    "#         return 3\n",
    "#     elif price >= 150 and price < 200:\n",
    "#         return 4\n",
    "#     elif price >= 200 and price < 250:\n",
    "#         return 5\n",
    "#     elif price >= 250 and price < 300:\n",
    "#         return 6\n",
    "#     elif price >= 300 and price < 350:\n",
    "#         return 7\n",
    "#     elif price >= 350 and price < 400:\n",
    "#         return 8\n",
    "#     elif price >= 400 and price < 450:\n",
    "#         return 9\n",
    "#     elif price >= 450 and price < 500:\n",
    "#         return 10\n",
    "#     elif price >= 500 and price < 550:\n",
    "#         return 11\n",
    "#     elif price >= 550 and price < 600:\n",
    "#         return 12\n",
    "#     elif price >= 600 and price < 650:\n",
    "#         return 13\n",
    "#     elif price >= 650 and price < 700:\n",
    "#         return 14\n",
    "#     else:\n",
    "#         return 15\n",
    "\n",
    "# #Applying transform method and assigning result to new column \"points_simplified\"\n",
    "# df_3 = df_3.assign(price_grouped = df_3['price'].apply(price_bins_2))\n",
    "# df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vrUrvbIUrgAc"
   },
   "outputs": [],
   "source": [
    "# # ALTERNATIVE BINS MODEL NUMBER: 3, Returns 92% Accuracy\n",
    "\n",
    "# def price_bins_3(price):\n",
    "#     if price < 75:\n",
    "#         return 1\n",
    "#     elif price >= 75 and price < 150:\n",
    "#         return 2\n",
    "#     elif price >= 150 and price < 225:\n",
    "#         return 3\n",
    "#     elif price >= 225 and price < 300:\n",
    "#         return 4\n",
    "#     elif price >= 300 and price < 375:\n",
    "#         return 5\n",
    "#     elif price >= 375 and price < 450:\n",
    "#         return 6\n",
    "#     elif price >= 450 and price < 525:\n",
    "#         return 7\n",
    "#     elif price >= 525 and price < 600:\n",
    "#         return 8\n",
    "#     elif price >= 600 and price < 675:\n",
    "#         return 9\n",
    "#     else:\n",
    "#         return 10\n",
    "\n",
    "# #Applying transform method and assigning result to new column \"points_simplified\"\n",
    "# df_3 = df_3.assign(price_grouped = df_3['price'].apply(price_bins_3))\n",
    "# df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "USXIDQPTpWhD"
   },
   "outputs": [],
   "source": [
    "# # ALTERNATIVE BINS MODEL NUMBER: 4, Returns 96% Accuracy\n",
    "\n",
    "# def price_bins_4(price):\n",
    "#     if price < 100:\n",
    "#         return 1\n",
    "#     elif price >= 100 and price < 200:\n",
    "#         return 2\n",
    "#     elif price >= 200 and price < 300:\n",
    "#         return 3\n",
    "#     elif price >= 300 and price < 400:\n",
    "#         return 4\n",
    "#     elif price >= 400 and price < 500:\n",
    "#         return 5\n",
    "#     elif price >= 500 and price < 600:\n",
    "#         return 6\n",
    "#     elif price >= 600 and price < 700:\n",
    "#         return 7\n",
    "#     else:\n",
    "#         return 8\n",
    "\n",
    "# # #Applying transform method and assigning result to new column \"points_simplified\"\n",
    "# df_3 = df_3.assign(price_grouped = df_3['price'].apply(price_bins_4))\n",
    "# df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DNU7Mo3h61LH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OfRcFzCp62xR"
   },
   "outputs": [],
   "source": [
    "# # ALTERNATIVE BINS MODEL NUMBER: 5, Returns 50% Accuracy\n",
    "\n",
    "# USE for histogram, IT shows the distribution is mainly consisting of wines less than $100\n",
    "\n",
    "# def price_bins_5(price):\n",
    "#     if price < 10:\n",
    "#         return 1\n",
    "#     elif price >= 10 and price < 20:\n",
    "#         return 2\n",
    "#     elif price >= 20 and price < 30:\n",
    "#         return 3\n",
    "#     elif price >= 30 and price < 40:\n",
    "#         return 4\n",
    "#     elif price >= 40 and price < 50:\n",
    "#         return 5\n",
    "#     elif price >= 50 and price < 60:\n",
    "#         return 6\n",
    "#     elif price >= 60 and price < 70:\n",
    "#         return 7\n",
    "#     elif price >= 70 and price < 80:\n",
    "#         return 8\n",
    "#     elif price >= 80 and price < 90:\n",
    "#         return 9\n",
    "#     elif price >= 90 and price < 100:\n",
    "#         return 10\n",
    "#     else:\n",
    "#         return 11\n",
    "\n",
    "# # #Applying transform method and assigning result to new column \"points_simplified\"\n",
    "# df_3 = df_3.assign(price_grouped = df_3['price'].apply(price_bins_5))\n",
    "# df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "me7qKsDs1uRm",
    "outputId": "0472676c-604b-4f14-bfd3-e4e4b792a602"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f048b27d2b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram\n",
    "# bins = df_3['price_grouped'].nunique()\n",
    "# ranges_scaled = df_3.loc[df_3['price_grouped'] > 1]\n",
    "\n",
    "\n",
    "sns.histplot(data=df_3, x=\"price_grouped\", element=\"step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qNWpauxvlMo"
   },
   "source": [
    "Converting the Pandas Dataframe to a Pyspark Dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGzHswBXg50j",
    "outputId": "b6571338-46c7-4332-f714-796321f05c81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+-------------+\n",
      "|         description|price|description_length|price_grouped|\n",
      "+--------------------+-----+------------------+-------------+\n",
      "|This tremendous 1...|235.0|               355|            8|\n",
      "|Ripe aromas of fi...|110.0|               318|            5|\n",
      "|Mac Watson honors...| 90.0|               280|            3|\n",
      "|This spent 20 mon...| 65.0|               386|            2|\n",
      "|This is the top w...| 66.0|               376|            2|\n",
      "|Deep, dense and p...| 73.0|               315|            3|\n",
      "|Slightly gritty b...| 65.0|               319|            2|\n",
      "|Lush cedary black...|110.0|               357|            5|\n",
      "|This re-named vin...| 65.0|               298|            2|\n",
      "|The producer sour...| 60.0|               307|            2|\n",
      "|Elegance, complex...| 80.0|               374|            3|\n",
      "|From 18-year-old ...| 48.0|               260|            2|\n",
      "|A standout even i...| 48.0|               289|            2|\n",
      "|This wine is in p...| 90.0|               303|            3|\n",
      "|With its sophisti...|185.0|               427|            7|\n",
      "|First made in 200...| 90.0|               277|            3|\n",
      "|This blockbuster,...|325.0|               276|           11|\n",
      "|Nicely oaked blac...| 80.0|               338|            3|\n",
      "|Coming from a sev...|290.0|               372|           10|\n",
      "|This fresh and li...| 75.0|               260|            3|\n",
      "+--------------------+-----+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pandas to Spark\n",
    "wine_DF = spark.createDataFrame(df_3)\n",
    "\n",
    "wine_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnSkv1o-oR9v",
    "outputId": "13c9b770-af46-4950-fda2-a8ec6a1bec51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------------+-------------+\n",
      "|description|price|description_length|price_grouped|\n",
      "+-----------+-----+------------------+-------------+\n",
      "|          0|    0|                 0|            0|\n",
      "+-----------+-----+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure no missing values were created when converting the DataFrame\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, isnull\n",
    "\n",
    "wine_DF.select([count(when(isnull(j), j)).alias(j) for j in wine_DF.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXYm8gqEjJH3"
   },
   "source": [
    "Feeding the NLP model to catagorize the wines into price points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mNyUIYQXp7af"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "# Create all the features to the data set\n",
    "wine_prices = StringIndexer(inputCol='price_grouped',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "L1RUBS6FR-eQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'description_length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gvpAP7aCp7-R"
   },
   "outputs": [],
   "source": [
    "# Create a and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[wine_prices, tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9uQ3YTrbp433"
   },
   "outputs": [],
   "source": [
    "# Fitting and Transforming the Pipeline\n",
    "cleaner = data_prep_pipeline.fit(wine_DF)\n",
    "cleaned = cleaner.transform(wine_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkdA8p83ZHhE",
    "outputId": "d1867749-354d-428e-de9f-bf8f8386feef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  6.0|(262145,[2701,160...|\n",
      "|  3.0|(262145,[11481,33...|\n",
      "|  2.0|(262145,[10077,26...|\n",
      "|  1.0|(262145,[1546,423...|\n",
      "|  1.0|(262145,[1546,171...|\n",
      "|  2.0|(262145,[8408,181...|\n",
      "|  1.0|(262145,[5561,196...|\n",
      "|  3.0|(262145,[4235,315...|\n",
      "|  1.0|(262145,[2306,316...|\n",
      "|  1.0|(262145,[3354,415...|\n",
      "|  2.0|(262145,[3848,939...|\n",
      "|  1.0|(262145,[3456,215...|\n",
      "|  1.0|(262145,[4176,894...|\n",
      "|  2.0|(262145,[8297,120...|\n",
      "|  5.0|(262145,[5847,160...|\n",
      "|  2.0|(262145,[1546,439...|\n",
      "| 10.0|(262145,[18911,55...|\n",
      "|  2.0|(262145,[2701,104...|\n",
      "|  8.0|(262145,[1546,544...|\n",
      "|  2.0|(262145,[3572,190...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show label and resulting features\n",
    "cleaned.select(['label', 'features']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4pKYGHGPVsS"
   },
   "source": [
    "Import the NaiveBayes module for the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "W5Key6f9ZVoc"
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# # Break data down into a training set and a testing set\n",
    "# training, testing = cleaned.randomSplit([0.7, 0.3])\n",
    "\n",
    "# # Create a Naive Bayes model and fit training data\n",
    "# nb = NaiveBayes()\n",
    "# predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "JB3qQD1yZfIA"
   },
   "outputs": [],
   "source": [
    "# Tranform the model with the testing data\n",
    "# test_results = predictor.transform(testing)\n",
    "# test_results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CBRHJPTPo2E"
   },
   "source": [
    "Use the Class Evaluator to return the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z9ij2aOvc5ct"
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# acc_eval = MulticlassClassificationEvaluator()\n",
    "# acc = acc_eval.evaluate(test_results)\n",
    "# print(\"Accuracy of model at predicting the wine's price was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA64mqkIUlfc"
   },
   "source": [
    "Cross Validation of the Classification Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Owt6RaCsUrUr",
    "outputId": "c1ca7ca9-6089-47b8-fad6-14efd7d25c8c"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-777b5dc5393e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Run cross validations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mnbcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbcvModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.0.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o281.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 212, 760dadb999d1, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2066)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions$lzycompute(MulticlassMetrics.scala:61)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.confusions(MulticlassMetrics.scala:52)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass$lzycompute(MulticlassMetrics.scala:66)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.labelCountByClass(MulticlassMetrics.scala:64)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:227)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure$lzycompute(MulticlassMetrics.scala:235)\n\tat org.apache.spark.mllib.evaluation.MulticlassMetrics.weightedFMeasure(MulticlassMetrics.scala:235)\n\tat org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.evaluate(MulticlassClassificationEvaluator.scala:178)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2066)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n"
     ]
    }
   ],
   "source": [
    "# Import additional modules\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Create initial Naïve Bayes model\n",
    "nb_2 = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Break data down into a training set and a testing set\n",
    "train, test = cleaned.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "nbparamGrid = (ParamGridBuilder()\n",
    "               .addGrid(nb_2.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "               .build())\n",
    "\n",
    "# Evaluate model\n",
    "nbevaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "# Create 3-fold CrossValidator\n",
    "nbcv = CrossValidator(estimator = nb_2,\n",
    "                    estimatorParamMaps = nbparamGrid,\n",
    "                    evaluator = nbevaluator,\n",
    "                    numFolds = 2)\n",
    "\n",
    "# Run cross validations\n",
    "nbcvModel = nbcv.fit(train)\n",
    "print(nbcvModel)\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "nbpredictions = nbcvModel.transform(test)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "print('Accuracy:', nbevaluator.evaluate(nbpredictions))\n",
    "# print('AUC:', BinaryClassificationMetrics(nbpredictions['label','prediction'].rdd).areaUnderROC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GckcBGnHix4z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPb5n7im8mSvqME3CTdxuDS",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1FCQhy1CgSuQPSkLHYFCng0ylqNg3w050",
   "name": "wine_prices.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
